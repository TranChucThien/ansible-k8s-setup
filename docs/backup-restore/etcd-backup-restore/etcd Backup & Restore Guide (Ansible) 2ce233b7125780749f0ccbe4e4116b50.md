# etcd Backup & Restore Guide (Ansible)

## **Má»¥c Lá»¥c**

- [Cluster Overview](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/tcthien/ansible/k8s-docker-hosts/ansible-k8s/docs/etcd-backup-restore/etcd-backup-restore.md#cluster-overview)
- [Use Cases: etcd vs Velero](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/tcthien/ansible/k8s-docker-hosts/ansible-k8s/docs/etcd-backup-restore/etcd-backup-restore.md#use-cases-etcd-vs-velero)
- [Manual Backup](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/tcthien/ansible/k8s-docker-hosts/ansible-k8s/docs/etcd-backup-restore/etcd-backup-restore.md#manual-backup)
- [Automated Backup](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/tcthien/ansible/k8s-docker-hosts/ansible-k8s/docs/etcd-backup-restore/etcd-backup-restore.md#automated-backup)
- [Restore Process](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/tcthien/ansible/k8s-docker-hosts/ansible-k8s/docs/etcd-backup-restore/etcd-backup-restore.md#restore-process)

## **Cluster Overview**

### **Current HA Cluster Setup**

| Component | IP Address | Role |
| --- | --- | --- |
| Master 1 | 192.168.10.138 | etcd + control plane |
| Master 2 | 192.168.10.139 | etcd + control plane |
| Master 3 | 192.168.10.140 | etcd + control plane |
| Worker 1 | 192.168.10.142 | Compute node |
| HAProxy Primary | 192.168.10.141 | Load balancer + Keepalived |
| HAProxy Backup | 192.168.10.143 | Backup load balancer |
| VIP | 192.168.10.100 | Virtual IP (Keepalived) |

### **etcd Cluster Architecture**

```
etcd Cluster (3 nodes - Quorum 2/3):
â”œâ”€â”€ k8s-master-1 (192.168.10.138) - Leader/Follower
â”œâ”€â”€ k8s-master-2 (192.168.10.139) - Leader/Follower
â””â”€â”€ k8s-master-3 (192.168.10.140) - Leader/Follower

Data Sync: Raft consensus protocol
```

## **Use Cases: etcd vs Velero**

### **Khi NÃ o DÃ¹ng etcd Backup?**

**âœ… etcd backup cho Disaster Recovery:**

- ğŸ”¥ Máº¥t hoÃ n toÃ n cluster (hardware failure, datacenter down)
- ğŸ—ï¸ Rebuild cluster tá»« Ä‘áº§u trÃªn infrastructure má»›i
- ğŸ”„ Major cluster migration (cloud provider, datacenter)
- ğŸ’¥ etcd corruption khÃ´ng sá»­a Ä‘Æ°á»£c

**âŒ KHÃ”NG dÃ¹ng etcd backup cho:**

- Restore individual applications â†’ DÃ¹ng Velero
- Backup application data/volumes â†’ DÃ¹ng Velero
- Accidental deletion â†’ DÃ¹ng Velero
- Cross-cluster workload migration â†’ DÃ¹ng Velero

### **So SÃ¡nh Nhanh**

| Scenario | Solution |
| --- | --- |
| Lost entire cluster | etcd backup |
| Accidental app deletion | Velero |
| Application rollback | Velero |
| Hardware failure | etcd backup |
| Migrate specific apps | Velero |
| Cluster migration | etcd backup + Velero |

## **Manual Backup**

### **etcd Backup Theory cho HA Cluster**

**Táº¡i sao chá»‰ cáº§n backup tá»« 1 etcd node:**

1. **Raft Consensus Protocol**:
    - etcd sá»­ dá»¥ng Raft Ä‘á»ƒ sync data giá»¯a cÃ¡c nodes
    - Táº¥t cáº£ nodes cÃ³Â **identical data**Â táº¡i má»i thá»i Ä‘iá»ƒm
    - Backup tá»« báº¥t ká»³ node nÃ o Ä‘á»u cho káº¿t quáº£ giá»‘ng nhau
2. **Leader Election**:
    - 1 node lÃ  Leader (handle writes)
    - 2 nodes lÃ  Followers (replicate data)
    - Data Ä‘Æ°á»£c replicate Ä‘á»“ng bá»™ trÆ°á»›c khi commit
3. **Backup Strategy**:
    - **Recommended**: Backup tá»« Leader node (performance tá»‘t nháº¥t)
    - **Alternative**: Backup tá»« báº¥t ká»³ Follower nÃ o
    - **Avoid**: Backup Ä‘á»“ng thá»i tá»« nhiá»u nodes (unnecessary)

```bash
# Kiá»ƒm tra Leader node
kubectl exec -n kube-system etcd-k8s-master-1 -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  endpoint status --write-out=table
```

![images/etcd-leader-status.png](images/etcd-leader-status.png)
*HÃ¬nh 1: Kiá»ƒm tra etcd leader status - Master 1 Ä‘ang lÃ  leader node*

### **Check etcd Health**

```bash
# Check all etcd members health
kubectl exec -n kube-system etcd-k8s-master-1 -- etcdctl \
  --endpoints=https://192.168.10.138:2379,https://192.168.10.139:2379,https://192.168.10.140:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  endpoint health --write-out=table

# Check member list and leader
kubectl exec -n kube-system etcd-k8s-master-1 -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  member list --write-out=table
```

![images/etcd-health-check.png](images/etcd-health-check.png)
*HÃ¬nh 2: Kiá»ƒm tra health status cá»§a táº¥t cáº£ etcd members trong cluster*

### **Create Backup**

```bash
# Create backup directory
sudo mkdir -p /opt/etcd-backup/manual

# Find available etcd pod (any master node)
ETCD_POD=$(kubectl get pods -n kube-system -l component=etcd --no-headers | head -1 | awk '{print $1}')
echo "Using etcd pod: $ETCD_POD"

# Create snapshot (backup tá»« báº¥t ká»³ etcd node nÃ o available)
kubectl exec -n kube-system $ETCD_POD -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /tmp/etcd-backup-$(date +%Y-%m-%d-%H-%M).db

# Copy to host
kubectl cp kube-system/$ETCD_POD:/tmp/etcd-backup-$(date +%Y-%m-%d-%H-%M).db \
  /opt/etcd-backup/manual/etcd-backup-$(date +%Y-%m-%d)-manual.db
```

### **Ansible Playbook**

```bash
ansible-playbook -i inventory-lab playbooks/21-backup-etcd.yml
```

![images/etcd-backup-playbook.png](images/etcd-backup-playbook.png)
*HÃ¬nh 3: Káº¿t quáº£ cháº¡y Ansible playbook backup etcd thÃ nh cÃ´ng*

### **Verify Backup**

```bash
 etcdutl snapshot status k8s-2025-12-19/etcd/snapshot.db  --write-out=table
```

![images/etcd-backup-verify.png](images/etcd-backup-verify.png)
*HÃ¬nh 4: Verify backup file integrity vá»›i etcdutl snapshot status*

### **Configure Crontab**

```bash
# Daily backup at 2AM
sudo crontab -e
# Add: 0 2 * * * /opt/etcd-backup/backup-script.sh >> /var/log/etcd-backup.log 2>&1
```

### **Ansible Playbook**

```bash
ansible-playbook -i inventory-lab playbooks/07-etcd-backup-automated.yaml
```

## **Restore Process**

### **âš ï¸ Cáº£nh BÃ¡o Quan Trá»ng**

**etcd restore CHá»ˆ dÃ¹ng khi:**

- Máº¥t hoÃ n toÃ n cluster
- Rebuild cluster má»›i
- Disaster recovery

**KHÃ”NG restore vÃ o cluster Ä‘ang cháº¡y!**

### **Restore Theory**

**Simplified Restore Process:**

1. **Restore snapshot trÃªn 1 master node**Â Ä‘á»ƒ táº¡o single-node cluster
2. **Start etcd**Â trÃªn master Ä‘Ã³ vá»›i restored data
3. **Join cÃ¡c masters khÃ¡c**Â vÃ o cluster nhÆ° bÃ¬nh thÆ°á»ng
4. etcd cluster sáº½ replicate data tá»›i cÃ¡c nodes má»›i

**âš ï¸ Quan trá»ng:**

- Chá»‰ cáº§n restore trÃªn 1 node (master Ä‘áº§u tiÃªn)
- CÃ¡c masters khÃ¡c join vÃ o nhÆ° cluster má»›i
- Data sáº½ Ä‘Æ°á»£c replicate tá»± Ä‘á»™ng

### **Pre-Restore**

```bash
# Verify backup integrity
etcdutl snapshot status /opt/etcd-backup/daily/etcd-backup-2025-12-19-02-00.db

# Document current state (if accessible)
kubectl get nodes
kubectl get ns
```

### 

### **Restore Steps (Simplified)**

### **Step 1: Chuáº©n Bá»‹ Master Má»›i**

```bash
# CÃ i Ä‘áº·t dependencies trÃªn master má»›i
ansible-playbook -i inventory playbooks/01-common.yaml --limit 192.168.10.138

```

### **Step 2: Restore vÃ  Init Cluster**

**Automated (Recommended):**

```bash
# Sá»­ dá»¥ng playbook tá»± Ä‘á»™ng
ansible-playbook -i inventory playbooks/restore-and-init.yml --limit 192.168.10.138

```

**Manual:**

```bash
# Copy backup file to master
scp k8s-2025-12-19/etcd/snapshot.db master@192.168.10.138:/tmp/

# Restore etcd snapshot
sudo etcdutl snapshot restore /tmp/snapshot.db \
  --data-dir /var/lib/etcd

# Init cluster má»›i
sudo kubeadm init \
  --pod-network-cidr=10.10.0.0/16 \
  --control-plane-endpoint=192.168.10.100:6443 \
  --upload-certs \
  --ignore-preflight-errors=DirAvailable--var-lib-etcd

# Setup kubectl
mkdir -p $HOME/.kube
sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Check cluster status
kubectl get all -A
```

### 

![images/etcd-restore-init-result.png](images/etcd-restore-init-result.png)
*HÃ¬nh 5: Káº¿t quáº£ thá»±c hiá»‡n etcd snapshot restore vÃ  kubeadm init thÃ nh cÃ´ng*

![images/etcd-restore-namespace-result.png](images/etcd-restore-namespace-result.png)
*HÃ¬nh 6: XÃ¡c nháº­n cÃ¡c namespace tá»« cluster cÅ© Ä‘Ã£ Ä‘Æ°á»£c restore thÃ nh cÃ´ng*

### **Important Notes**

**âš ï¸ Critical Points:**

1. **Fresh Infrastructure**: Restore trÃªn master má»›i, khÃ´ng sá»­a old cluster
2. **New Cluster Init**: DÃ¹ngÂ `kubeadm init`Â vá»›i restored etcd data
3. **Check Workloads**: LuÃ´n kiá»ƒm traÂ `kubectl get all -A`Â trÆ°á»›c vÃ  sau restore
4. **Automated**: Sá»­ dá»¥ng playbookÂ `restore-and-init.yml`Â cho Ä‘Æ¡n giáº£n

**ğŸ”„ Recovery Time:**Â 25-45 phÃºt

```

```

---

## **Best Practices**

### **Backup Strategy**

```
Production Setup:
â”œâ”€â”€ etcd backup (disaster recovery)
â”‚   â”œâ”€â”€ Daily: 2AM (30 days retention)
â”‚   â””â”€â”€ Pre-upgrade: Manual
â””â”€â”€ Velero (operational backup)
    â”œâ”€â”€ Applications: Per namespace
    â””â”€â”€ PV snapshots: Daily
```

### **Key Points**

- **etcd backup = Disaster recovery only**
- **Fresh infrastructure**: Restore trÃªn masters má»›i, khÃ´ng sá»­a old cluster
- **New cluster init**: DÃ¹ng kubeadm init vá»›i restored etcd data
- **Automated restore**: Sá»­ dá»¥ng playbook `restore-and-init.yml`
- **Check workloads**: LuÃ´n kiá»ƒm tra `kubectl get all -A` trÆ°á»›c vÃ  sau restore
- Test restore process regularly
- Store backups in multiple locations
- Document recovery procedures