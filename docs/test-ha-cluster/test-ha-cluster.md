# Kiá»ƒm thá»­ tÃ­nh "kháº£ dá»¥ng cao" cho cá»¥m Kubernetes

TÃ i liá»‡u nÃ y mÃ´ táº£ chi tiáº¿t toÃ n bá»™ quy trÃ¬nh kiá»ƒm thá»­ HA, bao gá»“m:

- Kiá»ƒm tra tráº¡ng thÃ¡i ná»™i bá»™ cá»§a Etcd (thÃ´ng qua `kubectl exec`)
- Triá»ƒn khai workload Ä‘á»ƒ theo dÃµi real-world behavior
- Thá»±c thi cÃ¡c ká»‹ch báº£n kiá»ƒm thá»­
- PhÃ¢n tÃ­ch hÃ nh vi Etcd quorum, API server failover vÃ  data-plane resiliency

**Environment:** Kubernetes v1.33 (Multi-Master, Stacked Etcd)

**Architecture:** 3 Control Plane Nodes (Masters), 1 Worker Node, 2 HAProxy/Keepalived Load Balancer

```markdown
## ğŸ—ï¸ Architecture Overview
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Virtual IP    â”‚
                    â”‚ 192.168.10.100  â”‚ â† Keepalived VIP
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                   â”‚                   â”‚
    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ HAProxy1  â”‚       â”‚ HAProxy2  â”‚       â”‚ HAProxy3  â”‚
    â”‚ (MASTER)  â”‚       â”‚ (BACKUP)  â”‚       â”‚ (BACKUP)  â”‚
    â”‚ .141      â”‚       â”‚ .143      â”‚       â”‚ (future)  â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚                   â”‚                   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                   â”‚                   â”‚
    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ Master1   â”‚       â”‚ Master2   â”‚       â”‚ Master3   â”‚
    â”‚ etcd1     â”‚       â”‚ etcd2     â”‚       â”‚ etcd3     â”‚
    â”‚ .138      â”‚       â”‚ .139      â”‚       â”‚ .140      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                   â”‚                   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Worker Nodes   â”‚
                    â”‚      .142         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# **PHáº¦N 1 â€” BASELINE HEALTH CHECK**

Äáº§u tiÃªn cáº§n xÃ¡c minh tráº¡ng thÃ¡i baseline cá»§a cá»¥m Kubernetes Ä‘á»ƒ Ä‘áº£m báº£o má»i thÃ nh pháº§n Ä‘ang hoáº¡t Ä‘á»™ng á»•n Ä‘á»‹nh.

---

## **1. Validate Node & System Pod Health**

**Expectation:** Cáº£ 3 etcd pods pháº£i á»Ÿ tráº¡ng thÃ¡i `Running` trÃªn 3 control-plane nodes khÃ¡c nhau.

```bash
# Kiá»ƒm tra danh sÃ¡ch Node
kubectl get nodes -o wide

# Kiá»ƒm tra cÃ¡c Pod etcd Ä‘ang cháº¡y trong namespace kube-system
kubectl get pods -n kube-system -l component=etcd -o wide
```

![Node Status](images/baseline/01-nodes-status.png)

![Etcd Pods Status](images/baseline/02-etcd-pods.png)

---

## **2. Kiá»ƒm tra Etcd (kubectl exec direct inspection)**

Thay vÃ¬ cÃ i `etcdctl` trÃªn host, cÃ³ thá»ƒ dÃ¹ng cÃ¡ch **exec trá»±c tiáº¿p vÃ o container cá»§a etcd**.

### **2.1. Kiá»ƒm tra danh sÃ¡ch etcd members**

```bash
kubectl exec -n kube-system etcd-k8s-master-1 -- etcdctl \
  --endpoints=https://k8s-master-1:2379,https://k8s-master-2:2379,https://k8s-master-3:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  member list --write-out=table
```

![Etcd Member List](images/etcd-tests/01-member-list.png)

### **2.2. Kiá»ƒm tra Leader vÃ  Raft health**

```bash
kubectl exec -n kube-system etcd-k8s-master-1 -- etcdctl \
  --endpoints=https://k8s-master-1:2379,https://k8s-master-2:2379,https://k8s-master-3:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  endpoint status --write-out=table
```

![Etcd Endpoint Status](images/etcd-tests/02-endpoint-status.png)

---

# **PHáº¦N 2 â€” DEPLOY WORKLOAD**

## **1. Deploy a sample workload**

```bash
kubectl create deployment nginx-ha-test \
  --image=nginx:latest \
  --replicas=3

kubectl expose deployment nginx-ha-test \
  --port=80 \
  --type=NodePort \
  --name=nginx-service
```

![Nginx Deployment](images/workload/01-nginx-deployment.png)

---

# **PHáº¦N 3 â€” SCENARIO 1: CONTROL-PLANE FAILURE & ETCD LEADER FAILOVER**

**Objective:** XÃ¡c minh hÃ nh vi Etcd leader failover, HAProxy, Kubernetes API khi máº¥t 1 master.

---

## **1. Action: Shutdown Master Node Ä‘ang giá»¯ vai trÃ² Etcd Leader**

```bash
ssh master@k8s-master-1 "sudo poweroff"
```

(*Thay `k8s-master-1` báº±ng leader thá»±c táº¿.*)

---

## **2. Post-failure Verification**

**Expected:** Leader má»›i pháº£i Ä‘Æ°á»£c báº§u, cluster váº«n hoáº¡t Ä‘á»™ng á»•n Ä‘á»‹nh.

SSH vÃ o má»™t Master cÃ²n sá»‘ng:

```bash
kubectl exec -n kube-system etcd-k8s-master-2 -- etcdctl \
  --endpoints=https://k8s-master-1:2379,https://k8s-master-2:2379,https://k8s-master-3:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  endpoint status --write-out=table
```

![Etcd Status After Failure](images/control-plane-failure/01-etcd-status-after-failure.png)

- Etcd bÃ¡o lá»—i vá»›i k8s-master-1 â†’ ÄÃ¢y lÃ  dáº¥u hiá»‡u Ä‘Ãºng khi Master Leader táº¯t
    
    ```bash
    rpc error: ... dial tcp 192.168.10.138:2379: connect: no route to host
    Failed to get the status of endpoint https://k8s-master-1:2379
    ```
    
- Káº¿t quáº£ Etcd endpoint status â€” Leader Ä‘Ã£ failover thÃ nh cÃ´ng: **Etcd Ä‘Ã£ báº§u leader má»›i lÃ  `k8s-master-2`.** Äiá»u nÃ y kháº³ng Ä‘á»‹nh **etcd quorum váº«n cÃ²n**:
    - Sá»‘ node etcd: 3
    - Sá»‘ node alive: 2
    
    â†’ **Cá»¥m etcd váº«n hoáº¡t Ä‘á»™ng tá»‘t**, khÃ´ng máº¥t dá»¯ liá»‡u, khÃ´ng máº¥t quyá»n ghi.

![Cluster Status After Failure](images/control-plane-failure/02-cluster-status.png)

- K8s Node Status: `k8s-master-1` bá»‹ shutdown â†’ chuyá»ƒn sang `NotReady`
- á»¨ng dá»¥ng Nginx HA Test váº«n cháº¡y á»•n â€” chá»©ng minh toÃ n cluster khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng

## **3. Káº¿t luáº­n Chung**

Sau khi shutdown 1 master (leader etcd), Ä‘Ã£ chá»©ng minh Ä‘Æ°á»£c:

### **âœ” 1. Etcd leader failover hoáº¡t Ä‘á»™ng chÃ­nh xÃ¡c**

- Node dead â†’ leader tá»± Ä‘á»™ng báº§u láº¡i
- Leader má»›i lÃ  `k8s-master-2`
- Quorum váº«n 2/3 nÃªn cá»¥m etcd váº«n ghi/Ä‘á»c Ä‘Æ°á»£c

### **âœ” 2. Control-plane HA váº«n hoáº¡t Ä‘á»™ng**

- kube-apiserver tiáº¿p tá»¥c phá»¥c vá»¥ qua HAProxy
- CÃ¡c control-plane component khÃ´ng bá»‹ giÃ¡n Ä‘oáº¡n

### **âœ” 3. Kubernetes cluster váº«n á»•n Ä‘á»‹nh**

- KhÃ´ng cÃ³ pod restart
- KhÃ´ng cÃ³ sá»± cá»‘ scheduling
- Service váº«n hoáº¡t Ä‘á»™ng

---

# **PHáº¦N 4 â€” SCENARIO 2: QUORUM LOSS / SPLIT-BRAIN PROTECTION**

**Objective:** Kiá»ƒm tra behavior khi Etcd máº¥t quorum (1/3 member cÃ²n sá»‘ng). 

---

## **1. Action: Shutdown thÃªm 1 control-plane node ná»¯a**

```bash
ssh user@k8s-master-2 "sudo poweroff"
```

---

## **2. Analysis**

- **Etcd Quorum Requirement:**
    
    ```
    Quorum = âŒŠN/2âŒ‹ + 1 = 2
    ```
    
    Hiá»‡n chá»‰ cÃ²n 1 node â†’ **máº¥t quorum**.
    
- **Etcd Behavior:**
    - Reject má»i write operation: vÃ¬ khÃ´ng thá»ƒ Ä‘áº¡t quorum Ä‘á»ƒ báº§u leader vÃ  replicate dá»¯ liá»‡u. Cluster rÆ¡i vÃ o tráº¡ng thÃ¡i read-only, khÃ´ng cháº¥p nháº­n cáº­p nháº­t.
    - Chá»‰ cho phÃ©p read háº¡n cháº¿:  Etcd váº«n há»— trá»£ read tá»« local data trÃªn tá»«ng node (serializable snapshot isolation), nhÆ°ng Kubernetes API server yÃªu cáº§u strongly-consistent reads nÃªn sáº½ fail khi khÃ´ng cÃ³ leader.
    - KhÃ´ng báº§u Ä‘Æ°á»£c leader: KhÃ´ng thá»ƒ thá»±c hiá»‡n election do thiáº¿u majority nodes (quorum = n/2 + 1), dáº«n Ä‘áº¿n tráº¡ng thÃ¡i "no leader" vÃ  cluster stuck.
- **Kubernetes API:** KhÃ´ng thá»ƒ phá»¥c vá»¥ request â†’ `Unable to connect to the server`.
- **Workload (Pods):** **VáºªN cháº¡y**  vÃ¬ data-plane (Kubelet, Container runtime) hoáº¡t Ä‘á»™ng Ä‘á»™c láº­p vá»›i control-plane.

---

## **3. Verification**

**Expected:** 

- Command sáº½ timeout â†’ ÄÃ¢y lÃ  behavior chÃ­nh xÃ¡c khi Etcd máº¥t quorum.
- Workload trÃªn worker node váº«n cÃ²n hoáº¡t Ä‘á»™ng

```bash
kubectl get nodes

curl 192.168.10.142:31324 #IP cá»§a worker node vÃ  service port

etcdctl \
  --endpoints=https://192.168.10.138:2379,https://192.168.10.139:2379,https://192.168.10.140:2379 \
  --cacert=etcd/ca.crt \
  --cert=etcd/healthcheck-client.crt \
  --key=etcd/healthcheck-client.key \
  endpoint status --write-out=table | less -S
  
 etcdctl \
  --endpoints=https://192.168.10.140:2379 \
  --cacert=etcd/ca.crt \
  --cert=etcd/healthcheck-client.crt \
  --key=etcd/healthcheck-client.key \
  endpoint health --cluster=false --write-out=table
  
  
  etcdctl \
  --endpoints=https://192.168.10.140:2379 \
  --cacert=etcd/ca.crt \
  --cert=etcd/healthcheck-client.crt \
  --key=etcd/healthcheck-client.key \
  get / --prefix --consistency=s --keys-only | grep service
```

![Kubectl Timeout](images/quorum-loss/01-kubectl-timeout.png)

- ÄÃºng theo spec: **Control-plane khÃ´ng hoáº¡t Ä‘á»™ng.**
    - kube-apiserver dÃ¹ng etcd Ä‘á»ƒ Ä‘á»c má»i thÃ´ng tin cluster â†’ Khi etcd máº¥t quorum â†’ kube-apiserver **khÃ´ng thá»ƒ phá»¥c vá»¥ request**
    - HAProxy/Keepalived váº«n hoáº¡t Ä‘á»™ng nhÆ°ng **API server khÃ´ng tráº£ lá»i**
- VÃ¬ **data-plane hoáº¡t Ä‘á»™ng Ä‘á»™c láº­p vá»›i control-plane â†’ workload váº«n cháº¡y**

![Workload Still Running](images/quorum-loss/02-workload-still-running.png)

![Nginx Still Accessible](images/quorum-loss/03-nginx-accessible.png)

![Etcd Health Check](images/quorum-loss/04-etcd-health-check.png)

![Etcd Serializable Read](images/quorum-loss/05-etcd-serializable-read.png)

Trong lá»‡nh cÃ³ tham sá»‘ `--consistency=s` (viáº¿t táº¯t cá»§a `serializable`). 

- **Linearizable (Máº·c Ä‘á»‹nh - `l`):** Khi Ä‘á»c, etcd node nÃ y (dÃ¹ lÃ  Follower) pháº£i liÃªn há»‡ vá»›i Leader Ä‘á»ƒ xÃ¡c nháº­n dá»¯ liá»‡u lÃ  má»›i nháº¥t. Náº¿u máº¥t káº¿t ná»‘i vá»›i Leader, lá»‡nh Ä‘á»c sáº½ tháº¥t báº¡i.
- **Serializable (`s`):** Etcd Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh Ä‘á»c trá»±c tiáº¿p dá»¯ liá»‡u hiá»‡n cÃ³ trong bá»™ nhá»› **cá»¥c bá»™ (local store)** cá»§a node Ä‘Ã³ mÃ  khÃ´ng cáº§n há»i Leader.
    - **Káº¿t luáº­n:** Viá»‡c lá»‡nh cháº¡y thÃ nh cÃ´ng vá»›i cá» `s` chá»©ng tá» nÃ³ Ä‘ang láº¥y dá»¯ liá»‡u Ä‘ang náº±m ngay trÃªn node `192.168.10.140`.

---

# **PHáº¦N 5 â€” RECOVERY**

**Objective:** XÃ¡c nháº­n cluster cÃ³ thá»ƒ phá»¥c há»“i khi sá»‘ lÆ°á»£ng member trá»Ÿ láº¡i Ä‘á»§ quorum.

---

## **1. Action: Bring Master Node #2 back online**

Báº­t láº¡i master node 2

---

## **2. Verify Control-Plane Recovery**

Expected: Pháº£i hoáº¡t Ä‘á»™ng láº¡i ngay khi quorum (2/3) Ä‘Æ°á»£c khÃ´i phá»¥c.

```bash
kubectl get nodes
```

![Cluster Recovered](images/recovery/01-cluster-recovered.png)

---

## **3. Verify Write Path Recovery**

```bash
kubectl scale deployment nginx-ha-test --replicas=5
```

![Scaling Test](images/recovery/02-scaling-test.png)

Scheduler táº¡o thÃªm Pod â†’ cluster Ä‘Ã£ fully recovered.

---

# **PHáº¦N 6 â€” Kiá»ƒm thá»­ Haproxy/KeepAlived**

ğŸ¯ **Má»¥c tiÃªu**

- Kiá»ƒm tra **failover** VIP tá»« MASTER â†’ BACKUP khi node Master lá»—i.
- Kiá»ƒm tra **failback** BACKUP â†’ MASTER khi Master phá»¥c há»“i.
- Äáº£m báº£o dá»‹ch vá»¥ (HAProxy, API Server, hoáº·c báº¥t ká»³ service nÃ o dÃ¹ng VIP) **khÃ´ng bá»‹ giÃ¡n Ä‘oáº¡n dÃ i**.

---

## **1. Kiá»ƒm tra tráº¡ng thÃ¡i ban Ä‘áº§u (Baseline)**

### 1.1 SSH vÃ o 2 node vÃ  kiá»ƒm tra tráº¡ng thÃ¡i Keepalived

**Expected:**

- Keepalived = active (running)
- VIP 192.168.10.100 pháº£i náº±m trÃªn card máº¡ng cá»§a node 141.
- KHÃ”NG cÃ³ VIP treen node 143

```bash
# Check which node owns the VIP
for node in 192.168.10.141 192.168.10.143; do
  echo "Checking VIP on $node:"
  ssh ha@$node "ip addr show | grep 192.168.10.100 || echo 'No VIP found'"
done

# Check HAProxy stats 
curl -I http://192.168.10.141:8404/stats
curl -I http://192.168.10.143:8404/stats
```

![VIP Baseline Check](images/haproxy-keepalived/01-vip-baseline.png)

---

### 1.2 Kiá»ƒm tra VRRP role qua log

**Expected:**

- Node 141 = `MASTER`
- Node 143 = `BACKUP`

```bash
ssh ha@192.168.10.141 "systemctl status keepalived"
ssh ha@192.168.10.143 "systemctl status keepalived"
```

![Master Status](images/haproxy-keepalived/02-master-status.png)

![Backup Status](images/haproxy-keepalived/03-backup-status.png)

---

## ğŸ”¥ **2. SCENARIO 1 â€” FAILOVER (MASTER â†’ BACKUP)**

**Má»¥c tiÃªu:** Khi MASTER cháº¿t, VIP nháº£y sang BACKUP

---

### 2.1 **Táº¯t node MASTER**

SSH tá»« node khÃ¡c:

```bash
ssh ha@192.168.10.141 
sudo poweroff
```

---

### 2.2 **Kiá»ƒm tra failover táº¡i node BACKUP**

```bash
ip a | grep 192.168.10.100
```

**Expected:**

- Node 143 *nháº­n Ä‘Æ°á»£c VIP*.
- Interface: thÆ°á»ng lÃ  `eth0` hoáº·c `ens33`.

![VIP Failover](images/haproxy-keepalived/04-vip-failover.png)

---

### 2.3 Kiá»ƒm tra Keepalived log trÃªn BACKUP

```bash
journalctl -u keepalived -n 20
```

**Expected:**

- MASTER STATE

![Backup Logs](images/haproxy-keepalived/05-backup-logs.png)

---

### 2.4 Kiá»ƒm tra káº¿t ná»‘i dá»‹ch vá»¥ (qua VIP)

Ping VIP:

```bash
ping -c4 192.168.10.100
```

![Ping VIP](images/haproxy-keepalived/06-ping-vip.png)

---

## 3. SCENARIO 2 â€” FAILBACK (BACKUP â†’ MASTER)

### 3.1 Kiá»ƒm tra log trÃªn MASTER

```bash
ssh ha@192.168.10.141 "systemctl status keepalived"
```

**Expected:**

```
MASTER STATE
```

![Master Recovered](images/haproxy-keepalived/07-master-recovered.png)

---

### 3.2 Kiá»ƒm tra VIP

```bash
#TrÃªn MASTER
ip a | grep 192.168.10.100
```

**Expected:** VIP quay láº¡i node 141.

---

```bash
#TrÃªn BACKUP:
ip a | grep 192.168.10.100
```

**Expected:** Node 143 khÃ´ng cÃ²n giá»¯ VIP.

![VIP Failback](images/haproxy-keepalived/08-vip-failback.png)

---

## 3.3 Kiá»ƒm tra dá»‹ch vá»¥ sau failback

```bash
kubectl get no
kubectl get all
```

**Expected:**

- Dá»‹ch vá»¥ váº«n liÃªn tá»¥c.
- KhÃ´ng máº¥t káº¿t ná»‘i.

![Final Status](images/haproxy-keepalived/09-final-status.png)

---