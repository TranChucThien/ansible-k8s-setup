# Kubernetes Single-Master Cluster (v2.0)

**Enterprise-grade Kubernetes cluster deployment with modular role-based architecture.**

Deploy a production-ready Kubernetes cluster with a single master node using modern Ansible automation. Perfect for development, testing, and small production environments.

## ğŸ—ï¸ Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Master Node             â”‚
â”‚    (Control Plane + etcd)       â”‚
â”‚  â€¢ API Server                   â”‚
â”‚  â€¢ Controller Manager           â”‚
â”‚  â€¢ Scheduler                    â”‚
â”‚  â€¢ etcd Database + etcdctl      â”‚
â”‚  â€¢ Calico CNI                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ Kubernetes API
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Worker Node 1   â”‚
    â”‚    (Compute)      â”‚
    â”‚  â€¢ kubelet        â”‚
    â”‚  â€¢ kube-proxy     â”‚
    â”‚  â€¢ containerd     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“– HÆ°á»›ng dáº«n sá»­ dá»¥ng

### BÆ°á»›c 1: Chuáº©n bá»‹ mÃ´i trÆ°á»ng
```bash
# Clone repository
git clone <repository-url>
cd project-k8s-single-master-v2

# CÃ i Ä‘áº·t Ansible (náº¿u chÆ°a cÃ³)
sudo apt update
sudo apt install ansible -y

# Kiá»ƒm tra version
ansible --version
```

### BÆ°á»›c 2: Cáº¥u hÃ¬nh inventory
```bash
# Sá»­a file inventory cho mÃ´i trÆ°á»ng cá»§a báº¡n
vim inventories/lab/hosts.ini

# Cáº­p nháº­t IP addresses
[k8s_masters]
k8s-master-1 ansible_host=YOUR_MASTER_IP

[k8s_workers]
k8s-worker-1 ansible_host=YOUR_WORKER_IP
```

### BÆ°á»›c 3: Kiá»ƒm tra káº¿t ná»‘i
```bash
# Test SSH connectivity
ansible all -i inventories/lab/hosts.ini -m ping

# Náº¿u lá»—i, kiá»ƒm tra:
# - SSH keys Ä‘Ã£ setup chÆ°a
# - Firewall cÃ³ block khÃ´ng
# - IP addresses cÃ³ Ä‘Ãºng khÃ´ng
```

### BÆ°á»›c 4: Deploy cluster
```bash
# Deploy complete cluster
ansible-playbook -i inventories/lab/hosts.ini playbooks/site.yml

# Hoáº·c chá»‰ deploy master (development)
ansible-playbook -i inventories/lab/hosts.ini playbooks/site-master-only.yml
```

### BÆ°á»›c 5: Verify installation
```bash
# SSH vÃ o master node
ssh master@YOUR_MASTER_IP

# Kiá»ƒm tra cluster
kubectl get nodes
kubectl get pods -A

# Test deploy má»™t pod
kubectl run test-pod --image=nginx
kubectl get pods
```

### BÆ°á»›c 6: Add workers (náº¿u cáº§n)
```bash
# ThÃªm worker má»›i
ansible-playbook -i inventories/lab/hosts.ini playbooks/site-add-workers.yml --limit k8s-worker-2

# Verify
kubectl get nodes
```

## ğŸš€ Quick Start

### Complete Cluster (Master + Workers)
```bash
# Deploy complete cluster from scratch
ansible-playbook -i inventories/lab/hosts.ini playbooks/site.yml
```

### Master Only (Development)
```bash
# Deploy only master node for development
ansible-playbook -i inventories/lab/hosts.ini playbooks/site-master-only.yml
```

### Add Workers to Existing Cluster
```bash
# Add specific workers
ansible-playbook -i inventories/lab/hosts.ini playbooks/site-add-workers.yml --limit k8s-worker-2

# Add multiple workers
ansible-playbook -i inventories/lab/hosts.ini playbooks/site-add-workers.yml --limit k8s-worker-1,k8s-worker-2
```

### Verify Installation
```bash
# Check cluster status
kubectl get nodes

# Expected output:
NAME           STATUS   ROLES           AGE   VERSION
k8s-master-1   Ready    control-plane   5m    v1.33.x
k8s-worker-1   Ready    <none>          3m    v1.33.x
```

## ğŸ“ Project Structure (v2.0)

```text
project-k8s-single-master-v2/
â”œâ”€â”€ inventories/                     # Multi-environment inventory
â”‚   â”œâ”€â”€ lab/                        # Lab environment (local VMs)
â”‚   â”‚   â”œâ”€â”€ hosts.ini
â”‚   â”‚   â””â”€â”€ group_vars/
â”‚   â”‚       â”œâ”€â”€ all.yml
â”‚   â”‚       â”œâ”€â”€ k8s_masters.yml
â”‚   â”‚       â”œâ”€â”€ k8s_workers.yml
â”‚   â”‚       â””â”€â”€ k8s_new_workers.yml
â”‚   â”œâ”€â”€ dev/                        # Dev environment (cloud)
â”‚   â””â”€â”€ prod/                       # Production environment
â”œâ”€â”€ roles/                          # Modular role-based architecture
â”‚   â”œâ”€â”€ common/                     # System setup + etcdctl
â”‚   â”œâ”€â”€ containerd/                 # Container runtime
â”‚   â”œâ”€â”€ kubernetes/
â”‚   â”‚   â”œâ”€â”€ install/               # Install k8s packages
â”‚   â”‚   â”œâ”€â”€ master/                # Initialize master + kubeadm config
â”‚   â”‚   â”œâ”€â”€ worker/                # Join workers
â”‚   â”‚   â”œâ”€â”€ join/                  # Generate join command
â”‚   â”‚   â””â”€â”€ reset/                 # Cluster cleanup
â”‚   â”œâ”€â”€ network/
â”‚   â”‚   â””â”€â”€ calico/                # CNI plugin
â”‚   â””â”€â”€ etcd/
â”‚       â””â”€â”€ backup/                # ETCD backup operations
â”œâ”€â”€ playbooks/
â”‚   â”œâ”€â”€ site.yml                   # Complete cluster deployment
â”‚   â”œâ”€â”€ site-master-only.yml       # Master-only deployment
â”‚   â”œâ”€â”€ site-add-workers.yml       # Add workers to existing cluster
â”‚   â”œâ”€â”€ site-add-specific-workers.yml # Add specific workers using k8s_new_workers group
â”‚   â”œâ”€â”€ site-backup.yml            # ETCD backup
â”‚   â”œâ”€â”€ site-backup-cron.yml       # Automated backup setup
â”‚   â””â”€â”€ archive/                   # Legacy playbooks (reference)
â”œâ”€â”€ logs/                          # Execution logs
â”œâ”€â”€ ansible.cfg                    # Ansible configuration
â”œâ”€â”€ README.md                      # This documentation
â””â”€â”€ ROLES.md                       # Role architecture guide
```

## ğŸ¯ Deployment Options

### Option 1: Complete Cluster
**Use case:** Production-like environment with master and workers

```bash
ansible-playbook -i inventories/lab/hosts.ini playbooks/site.yml
```

**What happens:**
1. âœ… Installs Docker and Kubernetes on all nodes
2. âœ… Installs etcdctl on master nodes
3. âœ… Initializes master with kubeadm config file
4. âœ… Installs Calico CNI networking
5. âœ… Joins all worker nodes to cluster
6. âœ… Configures kubectl access

### Option 2: Master-Only
**Use case:** Development/testing with single node

```bash
ansible-playbook -i inventories/lab/hosts.ini playbooks/site-master-only.yml
```

### Option 3: Add Workers Later
**Use case:** Scale existing cluster

```bash
# Add specific workers using --limit
ansible-playbook -i inventories/lab/hosts.ini playbooks/site-add-workers.yml --limit k8s-worker-2

# Or use k8s_new_workers group
# 1. Uncomment worker in inventories/lab/hosts.ini [k8s_new_workers] section
# 2. Run: ansible-playbook -i inventories/lab/hosts.ini playbooks/site-add-specific-workers.yml
```

## ğŸ“‹ Inventory Configuration

### Lab Environment (Local VMs)
```ini
# inventories/lab/hosts.ini
[k8s_masters]
k8s-master-1 ansible_host=192.168.10.138

[k8s_workers]
k8s-worker-1 ansible_host=192.168.10.142

# Temporary group for adding specific workers
[k8s_new_workers]
# k8s-worker-2 ansible_host=192.168.10.144  # Uncomment to add

[k8s_cluster:children]
k8s_masters
k8s_workers
k8s_new_workers
```


## ğŸ”§ Configuration Variables

### Global Settings (inventories/{env}/group_vars/all.yml)
```yaml
# Global variables for all hosts
kubernetes_version: "1.33"
pod_network_cidr: "10.10.0.0/16"
container_runtime: containerd
calico_version: "v3.28.0"

# ETCD backup settings
backup_dir: "/opt/etcd-backup"
backup_time: "0 2"  # Daily at 2:00 AM
backup_retention_days: 7
```

## ğŸ› ï¸ Operations

### Cluster Management
```bash
# Add workers to existing cluster
ansible-playbook -i inventories/lab/hosts.ini playbooks/site-add-workers.yml --limit k8s-worker-2

# Add workers using k8s_new_workers group
ansible-playbook -i inventories/lab/hosts.ini playbooks/site-add-specific-workers.yml
```

### ETCD Backup Operations
```bash
# Manual backup
ansible-playbook -i inventories/lab/hosts.ini playbooks/site-backup.yml

# Setup automated daily backups
ansible-playbook -i inventories/lab/hosts.ini playbooks/site-backup-cron.yml

# Custom backup schedule (3:00 AM, keep 14 days)
ansible-playbook -i inventories/lab/hosts.ini playbooks/site-backup-cron.yml \
  -e backup_time="0 3" -e backup_retention_days=14
```

### Testing & Validation
```bash
# Test connectivity
ansible all -i inventories/lab/hosts.ini -m ping

# Validate playbook syntax
ansible-playbook --syntax-check -i inventories/lab/hosts.ini playbooks/site.yml

# Dry run
ansible-playbook --check -i inventories/lab/hosts.ini playbooks/site.yml
```

## ğŸ”„ What's New in v2.0

### âœ… **Phase 1: Standardized Inventory**
- Multi-environment support (lab/dev/prod)
- Proper group_vars structure
- Scalable host management

### âœ… **Phase 2: Role-Based Architecture**
- Modular, reusable roles
- Clear separation of concerns
- Easy testing and maintenance
- Prepared for HA deployments

### âœ… **Phase 3: Kubeadm Lifecycle Management**
- Idempotent cluster initialization
- Kubeadm configuration templates

### âœ… **Phase 4: ETCD Backup System**
- Automated snapshot backups
- Cron job scheduling
- Backup retention policies
- etcdctl installation on masters

### âœ… **Phase 5: Organized Operations**
- Separated deployment vs operations playbooks
- Safety confirmations for destructive operations
- Maintenance operations (cordon/drain/uncordon)
- Node removal procedures
### ğŸ”§ **Technical Improvements**
- **Idempotent operations** - Safe to run multiple times
- **Fact-based communication** - No local file dependencies
- **Selective worker addition** - Add specific nodes with --limit
- **Comprehensive logging** - Detailed execution logs
- **Best practices** - Following Ansible and Kubernetes standards
- **Organized playbooks** - Clear separation of deploy vs ops

## ğŸ¯ Migration from v1.0

### Old Structure (v1.0)
```bash
# Old monolithic playbooks
ansible-playbook -i inventory playbooks/01-setup-common.yaml
ansible-playbook -i inventory playbooks/02-setup-master.yaml
ansible-playbook -i inventory playbooks/03-setup-worker.yaml
```

### New Structure (v2.0)
```bash
# New role-based approach
ansible-playbook -i inventories/lab/hosts.ini playbooks/site.yml
```

**Legacy playbooks are preserved in `playbooks/archive/` for reference.**

## âš ï¸ Troubleshooting

### Lá»—i thÆ°á»ng gáº·p

#### 1. SSH Connection Failed
```bash
# Kiá»ƒm tra SSH key
ssh-copy-id master@YOUR_MASTER_IP
ssh-copy-id worker@YOUR_WORKER_IP

# Hoáº·c sá»­ dá»¥ng password (khÃ´ng khuyáº¿n nghá»‹)
# ÄÃ£ cáº¥u hÃ¬nh trong group_vars
```

#### 2. Kubeadm Init Failed
```bash
# Reset vÃ  thá»­ láº¡i
sudo kubeadm reset -f
sudo rm -rf /etc/kubernetes/
sudo rm -rf ~/.kube/

# Cháº¡y láº¡i playbook
ansible-playbook -i inventories/lab/hosts.ini playbooks/site.yml
```

#### 3. Worker Join Failed
```bash
# Táº¡o láº¡i join token
sudo kubeadm token create --print-join-command

# Hoáº·c cháº¡y láº¡i add-worker playbook
ansible-playbook -i inventories/lab/hosts.ini playbooks/site-add-workers.yml --limit k8s-worker-1
```

#### 4. Pods Stuck in Pending
```bash
# Kiá»ƒm tra nodes
kubectl get nodes
kubectl describe nodes

# Kiá»ƒm tra CNI
kubectl get pods -n kube-system | grep calico
```

### Debug Commands
```bash
# Kiá»ƒm tra logs
journalctl -u kubelet -f
journalctl -u containerd -f

# Kiá»ƒm tra cluster health
kubectl cluster-info
kubectl get componentstatuses

# Kiá»ƒm tra network
kubectl get pods -n kube-system
kubectl logs -n kube-system <calico-pod-name>
```

## âš ï¸ Important Notes

### Limitations
- **Single Point of Failure**: Master node failure = cluster down
- **Development/Testing Focus**: Not suitable for high-availability production
- **No Load Balancer**: No VIP failover mechanism

### Security Considerations
- SSH key-based authentication recommended for production
- Network policies should be implemented
- RBAC configuration required for multi-tenant environments

### Performance Tuning
- Adjust resource limits based on workload
- Monitor etcd performance and disk I/O
- Consider node affinity for critical workloads

## ğŸ”— Related Projects

- [Multi-Master HA Setup](../project-k8s-multi-master-haproxy/README.md)
- [Multi-Master + Keepalived](../project-k8s-multi-master-haproxy-keepalived/README.md)
- [Troubleshooting Guide](troubleshooting/README.md)

## ğŸ“Š System Requirements

- **OS**: Ubuntu 24.04 LTS
- **Kubernetes**: v1.33.x
- **Container Runtime**: containerd
- **CNI Plugin**: Calico v3.28.0
- **Pod Network CIDR**: 10.10.0.0/16
- **Minimum Resources**: 2 CPU, 4GB RAM per node

## ğŸ¤ Contributing

1. Follow the role-based architecture
2. Update both code and documentation
3. Test in lab environment first
4. Maintain backward compatibility where possible

---

**Built with â¤ï¸ for the Kubernetes community**