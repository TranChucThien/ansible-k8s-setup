# Kubernetes Single Master Cluster

Deploy a simple Kubernetes cluster with single master node using Ansible.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Master Node   â”‚
â”‚ (Control Plane) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Worker Node   â”‚
â”‚   (Compute)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Files Structure

```
project-k8s-single-master/
â”œâ”€â”€ playbooks/
â”‚   â”œâ”€â”€ 01-common.yaml         # Common setup for all nodes
â”‚   â”œâ”€â”€ 02-master.yaml         # Master node initialization
â”‚   â”œâ”€â”€ 03-worker.yaml         # Worker nodes join cluster
â”‚   â”œâ”€â”€ clean-worker.yml       # Reset K8s configuration (all nodes)
â”‚   â”œâ”€â”€ site.yml               # Main deployment playbook
â”‚   â””â”€â”€ logs/                  # Execution logs with timestamps
â”œâ”€â”€ inventory                  # Cloud environment inventory
â”œâ”€â”€ inventory-lab              # Lab environment inventory
â”œâ”€â”€ ansible.cfg                # Ansible configuration with logging
â”œâ”€â”€ run-clean.sh               # Script for cleanup with logging
â””â”€â”€ README.md                  # This file
```

## ğŸš€ Quick Deploy

```bash
# Deploy complete cluster
ansible-playbook -i inventory playbooks/site.yml

# Or step by step
ansible-playbook -i inventory playbooks/01-common.yaml   # All nodes setup
ansible-playbook -i inventory playbooks/02-master.yaml   # Master init
ansible-playbook -i inventory playbooks/03-worker.yaml   # Workers join
```

## ğŸ§¹ Cleanup & Reset

```bash
# Reset all nodes (masters + workers)
ansible-playbook -i inventory playbooks/clean-worker.yml

# Or use script with logging
./run-clean.sh
```

## ğŸ“‹ Inventory Examples

### Cloud (AWS/EC2)
```ini
[masters]
47.129.50.197

[workers]
18.142.245.203

[masters:vars]
ansible_user=ubuntu
ansible_ssh_private_key_file=ansible-key.pem
```

### Lab Environment
```ini
[masters]
192.168.10.138

[workers]
192.168.10.142

[masters:vars]
ansible_user=master
ansible_ssh_pass=1
ansible_become_pass=1
```

## âœ… Verification

```bash
# Check cluster status
kubectl get nodes

# Expected output:
NAME           STATUS   ROLES           AGE   VERSION
k8s-master-1   Ready    control-plane   5m    v1.33.x
k8s-worker-1   Ready    <none>          3m    v1.33.x
```

## ğŸ”§ Configuration

- **Pod Network**: 10.10.0.0/16
- **CNI**: Calico v3.28.0
- **Kubernetes**: v1.33.x
- **Container Runtime**: containerd
- **Logging**: Enabled with timestamps

## ğŸ“Š Logging

All playbook executions are automatically logged:
- Main logs: `logs/ansible.log`
- Cleanup logs: `logs/k8s-reset-[timestamp].log`
- Deprecation warnings: Disabled

## âš ï¸ Limitations

- **Single Point of Failure**: Master node failure = cluster down
- **Development/Testing Only**: Not suitable for production
- **No HA**: No load balancer or VIP failover

For production, use [Multi-Master HA setup](../project-k8s-multi-master-haproxy/README.md).

## ğŸ”— Related

- [Multi-Master HA](../project-k8s-multi-master-haproxy/README.md)
- [Multi-Master + Keepalived](../project-k8s-multi-master-haproxy-keepalived/README.md)
- [Troubleshooting Guide](../docs/troubleshooting.md)