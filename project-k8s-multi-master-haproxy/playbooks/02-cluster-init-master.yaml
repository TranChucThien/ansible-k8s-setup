# This playbook sets up the FIRST Kubernetes master node with Calico networking
# ⚠️  WARNING: This only initializes the FIRST master for multi-master setup
# Run 03-join-masters.yaml afterwards to join additional masters
#
# It performs the following steps:
# 1. Initializes the Kubernetes cluster with controlPlaneEndpoint (for multi-master)
# 2. Configures kubectl access for the ansible user
# 3. Extracts the join command for worker nodes
# 4. Installs and configures the Calico CNI plugin

---
- name: Setup FIRST Kubernetes master with Calico network
  hosts: masters[0]  # Only target first master
  become: yes
  vars:
    haproxy_address: "192.168.10.141"
    pod_network_cidr: "10.10.0.0/16"
    calico_version: "v3.28.0"
  tasks:

    # Step 1: Initialize the Kubernetes cluster with kubeadm
    # Uses 10.10.0.0/16 as the pod network CIDR
    # Includes controlPlaneEndpoint for multi-master setup
    # Only runs if admin.conf doesn't exist (idempotency)
    - name: Step 1 - Initialize Kubernetes cluster with controlPlaneEndpoint
      command: kubeadm init --pod-network-cidr={{ pod_network_cidr }} --control-plane-endpoint={{ haproxy_address }}:6443 --upload-certs
      register: kubeadm_init_output
      args:
        creates: /etc/kubernetes/admin.conf

    # Step 2: Display the output from kubeadm init for debugging
    - name: Step 2 - Show kubeadm init output
      debug:
        var: kubeadm_init_output.stdout
      when: kubeadm_init_output.changed

    # Step 3: Create .kube directory for the ansible user to store kubectl config
    - name: Step 3 - Create .kube directory for user
      file:
        path: "/home/{{ ansible_user }}/.kube"
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0755'

    # Step 4: Copy the kubectl admin config to user's home directory
    - name: Step 4 - Copy admin.conf to user kube config
      copy:
        src: /etc/kubernetes/admin.conf
        dest: "/home/{{ ansible_user }}/.kube/config"
        remote_src: yes
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0644'


    




#######################################################
# Step 7: Install Calico operator (idempotent)
#######################################################

    # Check if Calico operator CRD exists
    - name: Check if Calico operator CRD exists
      command: kubectl get crd installations.operator.tigera.io
      environment:
        KUBECONFIG: "/home/{{ ansible_user }}/.kube/config"
      register: calico_operator_crd
      failed_when: false
      changed_when: false

    # Only create operator if CRD does NOT exist
    - name: Step 7 - Deploy Calico operator
      command: kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/{{ calico_version }}/manifests/tigera-operator.yaml
      environment:
        KUBECONFIG: "/home/{{ ansible_user }}/.kube/config"
      when: "'NotFound' in calico_operator_crd.stderr"
    # Step 8: Download the Calico custom resources manifest
    - name: Step 8 - Download Calico custom-resources file
      get_url:
        url: https://raw.githubusercontent.com/projectcalico/calico/{{ calico_version }}/manifests/custom-resources.yaml
        dest: /tmp/custom-resources.yaml

    # Step 9: Update the CIDR in custom resources to match cluster pod CIDR
    - name: Step 9 - Update CIDR in custom-resources.yaml
      replace:
        path: /tmp/custom-resources.yaml
        regexp: 'cidr: 192\.168\.0\.0/16'
        replace: 'cidr: {{ pod_network_cidr }}'

    # Step 10: Apply the modified Calico custom resources
    - name: Step 10 - Apply Calico custom resources
      command: kubectl apply -f /tmp/custom-resources.yaml
      environment:
        KUBECONFIG: "/home/{{ ansible_user }}/.kube/config"
      ignore_errors: yes

    # Step 11: Verify cluster node status
    # Retries up to 5 times with 10 second delay
    # Ensures at least the master node is in Ready state
    - name: Step 11 - Wait for all nodes to be ready (at least master)
      command: kubectl get nodes
      register: nodes_status
      environment:
        KUBECONFIG: "/home/{{ ansible_user }}/.kube/config"
      retries: 5
      delay: 10
      until: "'Ready' in nodes_status.stdout"
      check_mode: no

